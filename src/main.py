import asyncio
import mcp_use
from langchain_ollama.llms import OllamaLLM
from langchain_ollama.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
import tomllib
from mcp_use import MCPAgent, MCPClient
from pathlib import Path
import pandas as pd
from classifier import AnomalyDetector

project_root = Path(__file__).resolve().parent.parent
pyproject_file = project_root / "pyproject.toml"
server_config = project_root / "servers.json"


prompt_template_ticket = ChatPromptTemplate.from_template(
    """
Eres un agente de red m√≥vil. Se ha detectado una posible anomal√≠a en la red. Has de generar un email de aviso utilizando MCP y los siguientes datos deben de llegar al servidor:
- celda = {CellName}
- hora = {Time}
- probabilidad = {prob:.2f}
- mensaje = "Posible fallo en la red."
"""
)

prompt_template_accion = ChatPromptTemplate.from_template(
    """
Eres un agente de red m√≥vil encargado de ejecutar acciones autom√°ticas en celdas cuando se detecta una anomal√≠a en la regi√≥n alta. 
Tu tarea es redactar un mensaje profesional, breve y directo, indicando la acci√≥n que se ha realizado y la justificaci√≥n t√©cnica.

Datos de red de la celda {CellName} a la hora {Time}:
- PRBUsageUL: {PRBUsageUL}
- PRBUsageDL: {PRBUsageDL}
- meanThr_DL: {meanThr_DL}
- meanThr_UL: {meanThr_UL}
- maxThr_DL: {maxThr_DL}
- maxThr_UL: {maxThr_UL}
- meanUE_DL: {meanUE_DL}
- meanUE_UL: {meanUE_UL}
- maxUE_DL: {maxUE_DL}
- maxUE_UL: {maxUE_UL}
- maxUE_UL+DL: {maxUE_DL_UL}

Acci√≥n a comunicar: {accion}

Normas heur√≠sticas para referencia:
1. **Congesti√≥n UL**: PRBUsageUL > 5 * PRBUsageDL y meanThr_DL < 0.5 ‚Üí reasignar BW a UL o derivar tr√°fico a otras celdas.
2. **Celda infrautilizada**: PRBUsageUL < 1 y PRBUsageDL < 1 y maxUE_UL+DL < 3 ‚Üí incrementar BW asignado y ajustar broadcast.
3. **Fallo de contadores UL**: meanUE_UL < 0.05 y maxUE_UL > 0 ‚Üí reiniciar medici√≥n de contadores UL.

Formato esperado:
[Anomal√≠a en la celda {CellName} a la hora {Time}]
[Acci√≥n Realizada]: mensaje profesional que indique la acci√≥n ejecutada.
[Justificaci√≥n]: breve explicaci√≥n t√©cnica basada en los datos de red y las normas heur√≠sticas, dando valores concretos de los Datos de la red que respalden la acci√≥n en este caso. La explicaci√≥n ha de ser coherente con la acci√≥n realizada.

No incluyas introducciones, agradecimientos ni comentarios adicionales. Solo redacta la acci√≥n y la justificaci√≥n de manera profesional.
"""
)

def evaluar_regla(fila):
    """
    Eval√∫a la regla heur√≠stica sobre una fila de anomal√≠a (zona alta)
    y devuelve la acci√≥n que el sistema ya ha tomado autom√°ticamente.
    
    Devuelve:
    - regla_activa: descripci√≥n breve de la anomal√≠a detectada
    - accion: acci√≥n ejecutada autom√°ticamente
    """
    # Prioridad de reglas seg√∫n impacto
    if fila["PRBUsageUL"] > 5 * fila["PRBUsageDL"] and fila["meanThr_DL"] < 0.5:
        return (
            "Congesti√≥n UL",
            "Se ha reasignado BW a UL y/o se ha derivado tr√°fico a otras celdas para reducir congesti√≥n."
        )

    if fila["PRBUsageUL"] < 1 and fila["PRBUsageDL"] < 1 and fila["maxUE_UL+DL"] < 3:
        return (
            "Celda infrautilizada",
            "Se ha incrementado el ancho de banda asignado a la celda y ajustado el broadcast para mejorar utilizaci√≥n."
        )

    if fila["meanUE_UL"] < 0.05 and fila["maxUE_UL"] > 0:
        return (
            "Fallo de contadores UL",
            "Se ha reiniciado la medici√≥n de contadores UL para corregir la anomal√≠a."
        )

    # Si no cumple ninguna regla espec√≠fica
    return (
        "Anomal√≠a gen√©rica",
        "Se ha aplicado un reajuste general de par√°metros de la celda para corregir la anomal√≠a."
    )

async def agente_red(df, detector, llm, agent_ticket, low_th, high_th):
    X = df[detector.features]
    probs, _ = detector.predict(X)

    for i, prob in enumerate(probs):
        cell_name = df.iloc[i]["CellName"]
        time_val = df.iloc[i]["Time"]

        if prob < low_th:
            print(f"[OK] [{time_val}] {cell_name} ‚Üí Prob={prob:.2f} ‚Üí No anomal√≠a.")

        elif prob < high_th:
            print(f"[‚ö†] [{time_val}] {cell_name} ‚Üí Prob={prob:.2f} ‚Üí Zona intermedia, creando ticket...")
            prompt = prompt_template_ticket.format(
                prob=prob, CellName=cell_name, Time=time_val, region="intermedia"
            )
            result = await agent_ticket.run(prompt)
            print("Resultado del server:", result)

        else:  # Zona alta
            print(f"[üî•] [{time_val}] {cell_name} ‚Üí Prob={prob:.2f} ‚Üí Zona alta, acci√≥n autom√°tica...")

            fila = df.iloc[i]

            # Evaluar la regla en Python
            regla, accion = evaluar_regla(fila)

            # Formatear prompt para que el LLM solo redacte texto seg√∫n la regla
            prompt = prompt_template_accion.format(
                prob=prob,
                CellName=cell_name,
                Time=time_val,
                region="alta",
                regla=regla,
                accion=accion,
                PRBUsageUL=fila["PRBUsageUL"],
                PRBUsageDL=fila["PRBUsageDL"],
                meanThr_DL=fila["meanThr_DL"],
                meanThr_UL=fila["meanThr_UL"],
                maxThr_DL=fila["maxThr_DL"],
                maxThr_UL=fila["maxThr_UL"],
                meanUE_DL=fila["meanUE_DL"],
                meanUE_UL=fila["meanUE_UL"],
                maxUE_DL=fila["maxUE_DL"],
                maxUE_UL=fila["maxUE_UL"],
                maxUE_DL_UL=fila["maxUE_UL+DL"],
            )

            response = llm.invoke(prompt)
            print(response.content)

async def main():

    # Cargar configuraci√≥n
    with open(pyproject_file, "rb") as f:
        config = tomllib.load(f)

    #leer thresholds
    low_th = config["thresholds"]["low"]
    high_th = config["thresholds"]["high"]

    # Inicializar LLM
    use_chatgpt = config.get("openai", {}).get("use_chatgpt", False)
    if use_chatgpt:
        from langchain.chat_models import ChatOpenAI
        llm = ChatOpenAI(
            model_name=config["openai"]["model_name"],
            temperature=config["openai"]["temperature"]
        )
    else:
        llm = ChatOllama(
            model=config["agent"]["llm_model"],
            base_url="http://localhost:11434" 
        )

    model_path = project_root / config["classifier"]["model_path"]
    detector = AnomalyDetector(path_model=model_path)

    # Conectar cliente MCP
    client = MCPClient.from_config_file(server_config)

    # Crear agente
    agent_ticket = MCPAgent(
        llm=llm,
        client=client,
        max_steps=5,
        system_prompt="Eres asistente de detecci√≥n de anomal√≠as en redes m√≥viles."
    )

    print("üöÄ Cliente MCP iniciado")

    data_path = project_root /"data/ML-MATT-CompetitionQT1920_test.csv"
    df_to_label = pd.read_csv(data_path)

    muestras = df_to_label.sample(n=7, random_state=None)

    await agente_red(muestras, detector, llm, agent_ticket, low_th, high_th)

    # Cerrar sesiones MCP
    await client.close_all_sessions()

if __name__ == "__main__":
    asyncio.run(main())
    
